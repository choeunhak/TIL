{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "text_train의 타입: <class 'list'>\n",
      "text_train의 길이: 25000\n",
      "text_train[6]: \n",
      "b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n",
      "y_train[6]: \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"./data/aclImdb/train\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(type(reviews_train))\n",
    "print(\"text_train의 타입: {}\".format(type(text_train)))\n",
    "print(\"text_train의 길이: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]: \\n{}\".format(text_train[6]))\n",
    "print(\"y_train[6]: \\n{}\".format(y_train[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-724d38c1b8f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#<br>태그 제거\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"<br />\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\" \"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_train' is not defined"
     ]
    }
   ],
   "source": [
    "#<br>태그 제거\n",
    "print(text_train[5], \"\\n\")\n",
    "print(type(text_train[5]), \"\\n\")\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "print(text_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 수 확인\n",
    "import numpy as np\n",
    "print(\"클래스별 샘플 수 (훈련 데이터): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "reviews_train = load_files(\"./data/aclImdb/train\")\n",
    "reviews_test = load_files(\"./data/aclImdb/test\")\n",
    "\n",
    "# data는 text파일, target은 폴더 번호\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "# data에서 개행문자 삭제\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "\n",
    "print(\"학습 데이터의 문서 수: {}\".format(len(text_train)))\n",
    "print(\"테스트 데이터의 문서 수: {}\".format(len(text_test)))\n",
    "print(\"클래스별 샘플 수 (학습 데이터): {}\".format(np.bincount(y_train)))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터): {}\".format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "# CountVectorizer 객체 생성\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# 문자열 리스트를 토큰으로 분리하고, 어휘사전 구축\n",
    "vect.fit(bards_words)\n",
    "\n",
    "# 어휘사전은 vocabulary_ 속성에 저장됨\n",
    "print(\"어휘 사전의 크기: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"어휘 사전의 내용: \\n{}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW 생성\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "\n",
    "# 희소행렬로 표현(0이 많기 때문)\n",
    "# repr : 숫자 -> 문자열\n",
    "print(\"BOW: {}\".format(repr(bag_of_words)), \"\\n\")\n",
    "print(bag_of_words, \"\\n\")\n",
    "\n",
    "# numpy로 변환해야 볼 수 있음. 각 document의 bow 표현\n",
    "print(\"BOW의 밀집 표현: \\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영화 리뷰에 대한 bow\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"특성 개수: {}\".format(len(feature_names)))\n",
    "print(\"처음 20개 특성:\\n{}\".format(feature_names[:20]))\n",
    "\n",
    "#불용어 제거\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"불용어 개수: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"불용어 일부:\\n{}\".format(list(ENGLISH_STOP_WORDS)[:20]))\n",
    "\n",
    "# stop words=\"english\"라고 지정하면 내장된 불용어를 사용함\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"불용어가 제거된 X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDP 벡터라이저\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "vect1 = CountVectorizer().fit(corpus)\n",
    "tf = vect1.transform(corpus)\n",
    "\n",
    "feature_names = vect1.get_feature_names()\n",
    "print(\"Term:{}\".format(feature_names[:]))\n",
    "print(tf.toarray(), \"\\n\")\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(corpus)\n",
    "tfidf = vect2.transform(corpus)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-gram방식\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\n",
    "    'The fool doth think he is wise,',\n",
    "    'but the wise man knows himself to be a fool'\n",
    "]\n",
    "\n",
    "# (1, 1) 최소, 최대 단어수\n",
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 쌍의 단어로 구성\n",
    "cv = CountVectorizer(ngram_range=(2,2)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram부터 trigram까지 모두 포함\n",
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"변환된 데이터 (밀집 배열):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 단어별 stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stemmer 객체 생성\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for stem in example_words:\n",
    "    print(ps.stem(stem)) # stem 함수로 단어별 stemming 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize 에러 해결 부분\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전체를 stemming\n",
    "new_text = \"It is important to be very pythonly while you are pythoning with python.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "print(words)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "# CountVectorizer에 토큰을 생성하는 별도의 함수를 지정하고 함수 내에서 토큰화와 stemming을 실행\n",
    "vect = CountVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "vect.fit(['This swimmer likes swimming.'])\n",
    "\n",
    "sentence1 = vect.transform(['The swimmer likes swimming.'])\n",
    "sentence2 = vect.transform(['The swimmer swim. .'])\n",
    "\n",
    "print(vect.get_feature_names())\n",
    "print(sentence1.toarray())\n",
    "print(sentence2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming 기능을 추가한 BOW 생성\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"./data/aclImdb/train\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "reviews_test = load_files('./data/aclImdb/test')\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"테스트 데이터의 문서 수: {}\".format(len(text_test)))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "vect = CountVectorizer(tokenizer=tokenize, stop_words='english', token_pattern=u\"(?u)\\b\\w\\w+\\b\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n",
    "\n",
    "param_grid = {'C':[0.01, 0.1]}\n",
    "grid = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 매개변수: \", grid.best_params_)\n",
    "\n",
    "\n",
    "X_test = vect.transform(text_test)\n",
    "print(\"테스트 점수: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression으로 확인\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf=LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "pre=clf.predict(X_test)\n",
    "\n",
    "#정답률\n",
    "ac_score=accuracy_score(y_test, pre)\n",
    "print(\"정답률 = \", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
